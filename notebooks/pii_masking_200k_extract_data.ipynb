{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd28f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/u/a/n/anshumaan/phd_work/privacy_prompt_rewriting/universal-ner')\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import json\n",
    "import ast\n",
    "import random\n",
    "random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c637b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle data for english, french and german\n",
    "# say 10k samples each.\n",
    "import re\n",
    "\n",
    "def remove_brackets(text):\n",
    "    return re.sub(r'[\\[\\]]', '', text)    \n",
    "\n",
    "def word_cleaner(word):\n",
    "    word = word.replace('’','').replace(\"'s\", '').replace('.','').replace('?','').replace('!','')\n",
    "    return re.sub('\\W+','', word)\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    \"\"\"\n",
    "    Sample is a dict (json) object!\n",
    "    Entities: [Money, Name, Age, SSN, Credit Card Number, Zipcode, Date]\n",
    "    Negative sampling: Take 10k \n",
    "    \n",
    "    Preprocess all samples, sample by entity type. \n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"AMOUNT\": \"Money\",\n",
    "        \"FIRSTNAME\": \"Name\",\n",
    "        \"LASTNAME\": \"Name\",\n",
    "        \"MIDDLENAME\": \"Name\",\n",
    "        \"AGE\": \"Age\",\n",
    "        \"CREDITCARDNUMBER\": \"Credit Card Number\",\n",
    "        \"ZIPCODE\": \"Zipcode\",\n",
    "        \"DATE\": \"Date\",\n",
    "        \"SSN\": \"SSN\",\n",
    "    }\n",
    "    text = sample['unmasked_text']\n",
    "    privacy_mask = ast.literal_eval(sample['privacy_mask'])\n",
    "\n",
    "    # Intervene for names\n",
    "    firstNames = []\n",
    "    middleNames = []\n",
    "    lastNames = []\n",
    "    full_names = []\n",
    "\n",
    "    for key in privacy_mask:\n",
    "        if 'FIRSTNAME' in key:\n",
    "            firstNames.append(privacy_mask[key])\n",
    "        if 'LASTNAME' in key:\n",
    "            lastNames.append(privacy_mask[key])\n",
    "        if 'MIDDLENAME' in key:\n",
    "            middleNames.append(privacy_mask[key])\n",
    "\n",
    "    # print(firstNames)\n",
    "    # print(middleNames)\n",
    "    # print(lastNames)\n",
    "    # print(text)\n",
    "    \n",
    "    for name in firstNames:\n",
    "        words = text.split()\n",
    "        occurrences = []\n",
    "        for i, word in enumerate(words):\n",
    "            if name in word:\n",
    "                occurrences.append(i)\n",
    "        \n",
    "        temp_names = [name for i in range(len(occurrences))]\n",
    "\n",
    "        for i, k in enumerate(occurrences):\n",
    "            for middlename in middleNames:\n",
    "                if middlename in words[k+1]:\n",
    "                    temp_names[i] = temp_names[i] + \" \" + middlename\n",
    "                    occurrences[i] += 1\n",
    "\n",
    "        for i, k in enumerate(occurrences):\n",
    "            for lastname in lastNames:\n",
    "                if lastname in words[k+1]:\n",
    "                    temp_names[i] = temp_names[i] + \" \" + lastname\n",
    "                    occurrences[i] += 1\n",
    "        \n",
    "        for temp_name in temp_names: full_names.append(temp_name)\n",
    "\n",
    "    for name in middleNames:\n",
    "        flag = False\n",
    "        for full_name in full_names:\n",
    "            if name in full_name:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: continue\n",
    "\n",
    "        words = text.split()\n",
    "        occurrences = []\n",
    "        for i, word in enumerate(words):\n",
    "            if name in word:\n",
    "                occurrences.append(i)\n",
    "        \n",
    "        temp_names = [name for i in range(len(occurrences))]\n",
    "\n",
    "        for i, k in enumerate(occurrences):\n",
    "            for lastname in lastNames:\n",
    "                if lastname in words[k+1]:\n",
    "                    temp_names[i] = temp_names[i] + \" \" + lastname\n",
    "                    occurrences[i] += 1\n",
    "        \n",
    "        for temp_name in temp_names: full_names.append(temp_name)\n",
    "\n",
    "    for name in lastNames:\n",
    "        flag = False\n",
    "        for full_name in full_names:\n",
    "            if name in full_name:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: continue\n",
    "\n",
    "        words = text.split()\n",
    "        occurrences = []\n",
    "        for i, word in enumerate(words):\n",
    "            if name in word:\n",
    "                occurrences.append(i)\n",
    "        \n",
    "        temp_names = [name for i in range(len(occurrences))]\n",
    "        \n",
    "        for temp_name in temp_names: full_names.append(temp_name)\n",
    "\n",
    "    temp_dict = dict()\n",
    "    if len(full_names) > 0:\n",
    "        temp_dict[\"Name\"] = []\n",
    "        for full_name in full_names:\n",
    "            temp_dict[\"Name\"].append(full_name)\n",
    "\n",
    "    for key in privacy_mask:\n",
    "        # if 'NAME' in key: continue\n",
    "        value = privacy_mask[key]\n",
    "        formatted_key = remove_brackets(key).split('_')[0]\n",
    "        if formatted_key in mapping:\n",
    "            formatted_key = mapping[formatted_key]\n",
    "            \n",
    "        if formatted_key not in temp_dict: \n",
    "            temp_dict[formatted_key] = []\n",
    "\n",
    "        # Intervene for positive and negative samples.\n",
    "        if formatted_key in mapping.values():   \n",
    "            if formatted_key == \"Name\": continue\n",
    "            formatted_value = value\n",
    "            temp_dict[formatted_key].append(formatted_value)\n",
    "    \n",
    "    conversations = []\n",
    "    labels = list(temp_dict.keys())\n",
    "    # print(labels, len(labels))\n",
    "    \n",
    "    # Get the first label in with the text.\n",
    "    human_input_init = {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": f\"Text: {text}\"\n",
    "    }\n",
    "    model_output_init = { \n",
    "        \"from\": \"gpt\", \n",
    "        \"value\": \"I've read this text.\" \n",
    "    }\n",
    "    human_input = {\n",
    "        \"from\": \"human\", \n",
    "        \"value\": f\"What describes {labels[0]} in the text?\"\n",
    "    }\n",
    "    model_output = {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": f\"{temp_dict[labels[0]]}\".replace(\"'\", \"\\\"\")\n",
    "    }\n",
    "    conversations.append(human_input_init)\n",
    "    conversations.append(model_output_init)\n",
    "    conversations.append(human_input)\n",
    "    conversations.append(model_output)\n",
    "    \n",
    "    # Get other labels in.\n",
    "    for label in labels[1:]:\n",
    "        human_input = {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": f\"What describes {label} in the text?\",\n",
    "        }\n",
    "        model_output = {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": f\"{temp_dict[label]}\".replace(\"'\", \"\\\"\"),\n",
    "        }\n",
    "        conversations.append(human_input)\n",
    "        conversations.append(model_output)\n",
    "    \n",
    "    # Add in negative examples for chosen labels (otherwise the model tends to hallucinate)\n",
    "    entities = [\n",
    "        \"Money\", \"Name\", \"Age\", \"SSN\", \"Credit Card Number\", \"Zipcode\", \"Date\",\n",
    "    ]\n",
    "    for entity in entities:\n",
    "        if entity not in labels:\n",
    "            human_input = {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": f\"What describes {entity} in the text?\",\n",
    "            }\n",
    "            model_output = {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": \"[]\",\n",
    "            }\n",
    "            conversations.append(human_input)\n",
    "            conversations.append(model_output)\n",
    "\n",
    "    # Format for data pipeline.\n",
    "    sample_input = {\n",
    "        \"id\": \"\",\n",
    "        \"conversations\":conversations,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    \n",
    "    return sample_input\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ed145d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on ID: 243\n",
      "Error on ID: 4289\n",
      "Error on ID: 9025\n",
      "Error on ID: 9992\n",
      "Error on ID: 12182\n",
      "Error on ID: 13702\n",
      "Error on ID: 14871\n",
      "Error on ID: 14934\n",
      "Error on ID: 16456\n",
      "Error on ID: 16727\n",
      "Error on ID: 17372\n",
      "Error on ID: 19157\n",
      "Error on ID: 21439\n",
      "Error on ID: 23556\n",
      "Error on ID: 23611\n",
      "Error on ID: 25209\n",
      "Error on ID: 27759\n",
      "Error on ID: 27839\n",
      "Error on ID: 33590\n",
      "Error on ID: 33845\n",
      "Error on ID: 34749\n",
      "Error on ID: 36346\n",
      "Error on ID: 40276\n",
      "Error on ID: 43128\n",
      "##################################################\n",
      "25969 17508\n",
      "{'id': '', 'conversations': [{'from': 'human', 'value': 'Text: Reminder, Mr. Hirthe, you are scheduled to hold a Zoom meeting with both new and recurrent trauma patients today at 11:37am.'}, {'from': 'gpt', 'value': \"I've read this text.\"}, {'from': 'human', 'value': 'What describes Name in the text?'}, {'from': 'gpt', 'value': '[\"Hirthe\"]'}, {'from': 'human', 'value': 'What describes PREFIX in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes TIME in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Money in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Age in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes SSN in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Credit Card Number in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Zipcode in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Date in the text?'}, {'from': 'gpt', 'value': '[]'}], 'labels': ['Name', 'PREFIX', 'TIME']}\n",
      "{'id': '', 'conversations': [{'from': 'human', 'value': 'Text: Tyler_Ankunding60, The local government of Scotland has obtained the approvals necessary to alter the land use. You can access this document through 103.253.45.72 using the login credentials.'}, {'from': 'gpt', 'value': \"I've read this text.\"}, {'from': 'human', 'value': 'What describes USERNAME in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes STATE in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes IPV4 in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Money in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Name in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Age in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes SSN in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Credit Card Number in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Zipcode in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Date in the text?'}, {'from': 'gpt', 'value': '[]'}], 'labels': ['USERNAME', 'STATE', 'IPV4']}\n",
      "################################################## \n",
      "\n",
      "Error on ID: 1053\n",
      "Error on ID: 4144\n",
      "Error on ID: 4184\n",
      "Error on ID: 4879\n",
      "Error on ID: 6709\n",
      "Error on ID: 8121\n",
      "Error on ID: 10777\n",
      "Error on ID: 10793\n",
      "Error on ID: 11018\n",
      "Error on ID: 11379\n",
      "Error on ID: 15582\n",
      "Error on ID: 15952\n",
      "Error on ID: 16725\n",
      "Error on ID: 17838\n",
      "Error on ID: 17906\n",
      "Error on ID: 18189\n",
      "Error on ID: 19988\n",
      "Error on ID: 20241\n",
      "Error on ID: 22910\n",
      "Error on ID: 23164\n",
      "Error on ID: 24350\n",
      "Error on ID: 26639\n",
      "Error on ID: 26694\n",
      "Error on ID: 26904\n",
      "Error on ID: 27511\n",
      "Error on ID: 29134\n",
      "Error on ID: 31112\n",
      "Error on ID: 31503\n",
      "Error on ID: 36128\n",
      "Error on ID: 36203\n",
      "Error on ID: 38353\n",
      "Error on ID: 42257\n",
      "Error on ID: 50683\n",
      "Error on ID: 50684\n",
      "##################################################\n",
      "31792 20991\n",
      "{'id': '', 'conversations': [{'from': 'human', 'value': \"Text: Betr.: Mitarbeiterverhalten in South Willardfurt. Senden Sie Vorfallsberichte und die Mitarbeiter-Sozialversicherungsnummern 756.6252.5672 aus Provence-Alpes-Côte d'Azur bis 7/31 an Louisa63@gmail.com.\"}, {'from': 'gpt', 'value': \"I've read this text.\"}, {'from': 'human', 'value': 'What describes CITY in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes SSN in the text?'}, {'from': 'gpt', 'value': '[\"756.6252.5672\"]'}, {'from': 'human', 'value': 'What describes STATE in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Date in the text?'}, {'from': 'gpt', 'value': '[\"7/31\"]'}, {'from': 'human', 'value': 'What describes EMAIL in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Money in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Name in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Age in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Credit Card Number in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Zipcode in the text?'}, {'from': 'gpt', 'value': '[]'}], 'labels': ['CITY', 'SSN', 'STATE', 'Date', 'EMAIL']}\n",
      "{'id': '', 'conversations': [{'from': 'human', 'value': 'Text: Ein[e] Two-spirit person aus Mittieworth zeigte Symptome einer Dissoziativen Identitätsstörung.'}, {'from': 'gpt', 'value': \"I've read this text.\"}, {'from': 'human', 'value': 'What describes GENDER in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes CITY in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Money in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Name in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Age in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes SSN in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Credit Card Number in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Zipcode in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Date in the text?'}, {'from': 'gpt', 'value': '[]'}], 'labels': ['GENDER', 'CITY']}\n",
      "################################################## \n",
      "\n",
      "Error on ID: 3822\n",
      "Error on ID: 4464\n",
      "Error on ID: 6222\n",
      "Error on ID: 6936\n",
      "Error on ID: 9275\n",
      "Error on ID: 10082\n",
      "Error on ID: 10847\n",
      "Error on ID: 12576\n",
      "Error on ID: 15178\n",
      "Error on ID: 15548\n",
      "Error on ID: 15892\n",
      "Error on ID: 17624\n",
      "Error on ID: 18090\n",
      "Error on ID: 20588\n",
      "Error on ID: 22356\n",
      "Error on ID: 22635\n",
      "Error on ID: 23106\n",
      "Error on ID: 27526\n",
      "Error on ID: 27994\n",
      "Error on ID: 28550\n",
      "Error on ID: 29522\n",
      "Error on ID: 30880\n",
      "Error on ID: 32094\n",
      "Error on ID: 32367\n",
      "Error on ID: 32931\n",
      "Error on ID: 36003\n",
      "Error on ID: 39707\n",
      "Error on ID: 42253\n",
      "Error on ID: 43647\n",
      "Error on ID: 47058\n",
      "Error on ID: 51485\n",
      "Error on ID: 52052\n",
      "Error on ID: 55608\n",
      "Error on ID: 57464\n",
      "Error on ID: 58741\n",
      "Error on ID: 58749\n",
      "Error on ID: 60283\n",
      "Error on ID: 60739\n",
      "Error on ID: 61639\n",
      "##################################################\n",
      "37495 24424\n",
      "{'id': '', 'conversations': [{'from': 'human', 'value': \"Text: Cher Nico, nous avons un besoin urgent de votre expertise en droit de l'environnement. Nous sommes confrontés à un éventuel litige concernant la propriété située au 3636 Belmont Road.\"}, {'from': 'gpt', 'value': \"I've read this text.\"}, {'from': 'human', 'value': 'What describes Name in the text?'}, {'from': 'gpt', 'value': '[\"Nico\"]'}, {'from': 'human', 'value': 'What describes BUILDINGNUMBER in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes STREET in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Money in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Age in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes SSN in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Credit Card Number in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Zipcode in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Date in the text?'}, {'from': 'gpt', 'value': '[]'}], 'labels': ['Name', 'BUILDINGNUMBER', 'STREET']}\n",
      "{'id': '', 'conversations': [{'from': 'human', 'value': 'Text: Le nombre de cas selon le Male to female transgender woman du patient dans la région de Shropshire est alarmant. Un examen plus approfondi des données est nécessaire.'}, {'from': 'gpt', 'value': \"I've read this text.\"}, {'from': 'human', 'value': 'What describes GENDER in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes COUNTY in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Money in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Name in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Age in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes SSN in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Credit Card Number in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Zipcode in the text?'}, {'from': 'gpt', 'value': '[]'}, {'from': 'human', 'value': 'What describes Date in the text?'}, {'from': 'gpt', 'value': '[]'}], 'labels': ['GENDER', 'COUNTY']}\n",
      "################################################## \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3107480cb6b44cc6991aada8b9e27a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7a81673a90497cba1b4fcedabe67a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f983c14e68424296089e0bbc25aa84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = [\n",
    "    \"english_pii_43k.jsonl\",\n",
    "    \"german_pii_52k.jsonl\",\n",
    "    \"french_pii_62k.jsonl\",\n",
    "]\n",
    "# Entities: [Money, Name, Age, SSN, Credit Card Number, Zipcode, Date]\n",
    "entities = [\n",
    "    \"Money\", \"Name\", \"Age\", \"SSN\", \"Credit Card Number\", \"Zipcode\", \"Date\",\n",
    "]\n",
    "train_set = []\n",
    "test_set = []\n",
    "val_set = []\n",
    "for name in names:\n",
    "    processed = []\n",
    "    file_path = f\"/nobackup3/divyam/data/pii-masking-200k/{name}\"\n",
    "    content = read_jsonl(file_path)\n",
    "    random.shuffle(content)\n",
    "    # with ProcessPoolExecutor() as executor:\n",
    "    #     for result in tqdm(\n",
    "    #         executor.map(preprocess_sample, content), total=len(content)\n",
    "    #     ):\n",
    "    #         processed.append(result)\n",
    "\n",
    "    for i, sample in enumerate(content):\n",
    "        try:\n",
    "            result = preprocess_sample(sample)\n",
    "            processed.append(result)\n",
    "        except:\n",
    "            print(\"Error on ID:\", i)\n",
    "    positive_samples = []\n",
    "    negative_samples = []\n",
    "    for sample in processed:\n",
    "        a = set(sample['labels'])\n",
    "        b = entities\n",
    "        if a.intersection(b): positive_samples.append(sample)\n",
    "        else: negative_samples.append(sample)\n",
    "    \n",
    "    print(\"#\"*50)\n",
    "    print(len(positive_samples), len(negative_samples))\n",
    "    print(positive_samples[-1])\n",
    "    print(negative_samples[-1])\n",
    "    print(\"#\"*50,'\\n')\n",
    "\n",
    "    def train_val_test_split(samples):\n",
    "        a = samples[:int(0.7*len(samples))]\n",
    "        b = samples[int(0.7*len(samples)):int(0.8*len(samples))]\n",
    "        c = samples[int(0.8*len(samples)):]\n",
    "        return a, b, c\n",
    "    \n",
    "    train_positive, val_positive, test_positive = train_val_test_split(positive_samples)\n",
    "    train_negative, val_negative, test_negative = train_val_test_split(negative_samples)\n",
    "    train_set.extend(train_positive + train_negative)\n",
    "    test_set.extend(test_positive + test_negative)\n",
    "    val_set.extend(val_positive + val_negative)\n",
    "    \n",
    "#     processed = positive_samples + negative_samples\n",
    "#     final_processed.extend(processed)\n",
    "    \n",
    "for i in tqdm(range(len(train_set)), total=len(train_set)):\n",
    "    train_set[i][\"id\"] = f\"{i}\"\n",
    "\n",
    "for i in tqdm(range(len(val_set)), total=len(val_set)):\n",
    "    val_set[i][\"id\"] = f\"{i}\"\n",
    "\n",
    "for i in tqdm(range(len(test_set)), total=len(test_set)):\n",
    "    test_set[i][\"id\"] = f\"{i}\"\n",
    "    \n",
    "with open('/nobackup3/divyam/data/pii-masking-200k/pii_masking_200k_en_fr_de_train_v7.json', 'w') as fp:\n",
    "    json.dump(train_set, fp, indent=2)\n",
    "\n",
    "with open('/nobackup3/divyam/data/pii-masking-200k/pii_masking_200k_en_fr_de_val_v7.json', 'w') as fp:\n",
    "    json.dump(val_set, fp, indent=2)\n",
    "    \n",
    "with open('/nobackup3/divyam/data/pii-masking-200k/pii_masking_200k_en_fr_de_test_v7.json', 'w') as fp:\n",
    "    json.dump(test_set, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8db7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08610105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
